\chapter{Globally Convex Segmentation}
\label{ch:gcs}
Numerical experiments in the two models, GAC and ACWE, show that there is certainly room for improvement from the modelling perspective. Of particular concern are the models knack for finding and getting stuck at local minima (by design, in case of the GAC model), and therefore the solution attained is dependent on the initial contour. These issues are address by Chan, Esedo\={g}lu, and Nikolova \cite{chan2006algorithms}, and their ideas further refined by Bresson et al. \cite{bresson2007fast}, in what they referred to as the convexification of the ACWE model and the unification of two models we have considered.

Before going in depth, we should explain that in this chapter our viewpoint on the segmentation problem shifts somewhat. In the previous models, the viewpoint was mainly on an active contours and its evolution. However in the GCS model, the viewpoint is centred about indicator functions (of sets). Keep in mind that for 2-phase segmentation, any subset $\Sigma \subset \Omega$, and equivalently the corresponding indicator function $\ind_\Sigma$, defines a segmentation of the image domain. In what follows, the goal will arriving at an indicator function $u(x) = \ind_\Sigma(x)$, and the piecewise constant solution $\widetilde u(x) = c_1 \ind_\Sigma(x) + c_2 (1 - \ind_\Sigma(x))$. In other words, the problem may instead be framed as optimizating over functions that take only two values to find the best approximation to a given image $f$.

But first it is important to understand the nature of the non-convexity in the ACWE model. Restating the optimization problem in terms of indicator functions, we have 
\begin{align}
\min_{\substack{\Sigma\subset\Omega \\ 
		u(x) = \ind_\Sigma(x)}} 
\left\{\eacwe(u, c_1, c_2; \lambda)
= \int_{\Omega} \abs{\nabla u} \dx 
+ \int_\Omega u(c_1 - f)^2   + (1-u)(c_2 - f)^2 \dx 
\right\}.
\label{eq:eacwe2}
\end{align}
Observe that the function set we are optimizing over is not convex. For instance suppose $\Sigma_1, \Sigma_2 \subset \Omega$, $\Sigma_1 \cap \Sigma_2 = \emptyset$, $\Omega \setminus (\Sigma_1 \cup \Sigma_2) \neq \emptyset$ and set $u_1 = \ind_{\Sigma_1}$ and $u_2 = \ind_{\Sigma_2}$. Then any convex combination $w = ku_1 + (1-k)u_2$, $k \in (0,1)$, would be a function that takes on three values. 

In the next section are two key theorems which will allow that constraint to be relaxed, i.e. allow $u$ to take on the continuum of values between 0 and 1, and lead to an equivalent but convex minimization problem.

\section{A unified GAC \& ACWE convex segmentation model}
We first start the key observation that in choosing a non-compactly support $\delta_\epsilon$ in the gradient descent evolution \eqref{eq:acwe_el}, the following will have the same steady state solutions: 
\begin{align*}
\phi_t = \Div\left(\frac{\nabla \phi}{\abs{\nabla \phi}} \right) 
- \lambda (c_1 - f)^2 + \lambda (c_2 - f)^2 .
\end{align*}
This in turn is the gradient descent equation the following energy:
\begin{align}
E(\phi, c_1, c_2) 
= \int_\Omega \abs{\nabla \phi} \dx 
+ \lambda \int_\Omega \big[ (c_1 - f)^2 - (c_2 - f)^2 \big] \phi \dx ,
\label{eq:mod_acwe}
\end{align}
which gets us to the first of two key theorems from \cite{chan2006algorithms}.

\begin{thm}
	For any given fixed $c_1, c_2 \in \mathbf{R}$, a global minimizer for $\eacwe(\cdot, c_1, c_2; \lambda)$ can be found by carrying out the following convex minimization: 
	\begin{align}
	\min_{0\leq u \leq 1}\left\{
		\widetilde E_{\textrm{ACWE}} (u, c_1, c_2; \lambda)
		=
		\int_{\Omega} \abs{\nabla u} \dx 
	+ \lambda\int_{\Omega} \big[ (c_1 - f(x) )^2 - (c_2 - f(x))^2 \big] u(x) \dx
	\right\}
	\label{eq:thm1}
	\end{align}
	and then setting $\Sigma = \{ x \mid u(x) \geq \mu \}$ for a.e. $\mu \in [0, 1]$.
	\label{thm:1}
\end{thm}
The theorem above links the two energies, $\eacwe$ and $\widetilde E_\textrm{ACWE}$,
and guarantees that any global minimum of \eqref{eq:mod_acwe} is only a thresholding step away from a global minimum of the ACWE model. The next theorem is but one way to tackle \eqref{eq:thm1}.

\begin{thm}
	Let $r(x) \in L^\infty(\Omega)$. Then the convex, constrained minimization problem
	\begin{align*}
	\min_{0 \leq u \leq 1} \int_{\Omega} \abs{\nabla u} \dx + \lambda \int_{\Omega} r(x) u \dx 
	\end{align*}
	has the same set of minimizers as the following convex, unconstrained minimization problem:
	\begin{align*}
	\min_u \int_{\Omega} \abs{\nabla u} \dx + \lambda \int_{\Omega} r(x) u + \alpha \nu(u) \dx 
	\end{align*}
	where $\nu(\xi) = \max\{ 0 , 2\abs{\xi - \frac{1}{2} } - 1\}$, provided that $\alpha > \frac{\lambda}{2}\norm{r(x)}_{L^\infty(\Omega)}$.
	\label{thm:2}
\end{thm}
The term $\alpha\nu(u)$ is an exact penalty term \cite{hiriart1993convexI,hiriart1993convexII}. The advantage of this new unconstrained formulation is that it is quite straightforward to derive the Euler-Lagrange equation and solve by gradient descent. But before doing so, we will add one more modification to give a unified globally convex segmentation (GCS) model.

Per Bresson et al. \cite{bresson2007fast}, building on the work of \cite{chan2006algorithms}, they propose minimization of the energy 
\begin{align}
\egcs(u, c_1, c_2; \lambda) = 
\int_{\Omega} g(x)  \abs{\nabla u} \dx 
+ \lambda\int_{\Omega} \big[ (c_1 - f(x) )^2 - (c_2 - f(x))^2 \big] u(x) \dx,
\end{align}
where $g$ is an edge indicator function as in \Cref{ch:gac}. Furthermore, if $u$ is an indicator function, then
\begin{align*}
\egcs(\ind_\Sigma(x), c_1, c_2; \lambda) 
&= \int_{\Omega} g(x)  \abs{\nabla \ind_\Sigma(x)} \dx 
+ \lambda\int_{\Omega} \big[ (c_1 - f(x) )^2 - (c_2 - f(x))^2 \big] \ind_\Sigma(x) \dx
\\
&= \egac(\partial \Sigma) +  \lambda\int_{\Omega} \big[ (c_1 - f(x) )^2 - (c_2 - f(x))^2 \big] \ind_\Sigma(x) \dx
\end{align*}
reduces to the familiar GAC energy but now subject to an ACWE-type fitting energy constraint. Finally, also note that $\egcs$ satisfies statements same as those in Theorem \ref{thm:1} and \ref{thm:2}, but with $\int g\abs{\nabla u}$ in place of $\int \abs{\nabla u}$.

The next section will not be just one, but two numerical algorithms for image segmentation under the GCS model. In addition to the usual Euler-Lagrange equation and gradient descent, we will also discuss application of the Split Bregman method \cite{goldstein2010geometric,goldstein2009split} to this segmentation model.

\section{Minimization of the GCS model: gradient descent and the Split Bregman}
\subsection{Euler-Lagrange for the GCS model}
For this subsection, we are considering the unconstrained minimization problem 
\begin{align*}
\min_u \underbrace{\int_{\Omega } g\abs{\nabla u} \dx 
+ \int_{\Omega} \lambda \overbrace{\big[ (c_1 - f)^2 - (c_2 - f)^2 \big]}^{=r(x)} u + \alpha \nu(u) \dx }_{=\widetilde E_{\textrm{GCS}}(u)}
\end{align*}
($\egcs$ with an exact penalty term, see Theorem \ref{thm:2}).
Fixing $c_1, c_2$ and $\lambda$, we have
\begin{align*}
\dd{}{\gamma} \widetilde E_\textrm{GCS}(u + \gamma h)
\bigg\rvert_{\gamma = 0} 
&= \int_{\Omega} g \frac{\nabla u}{\abs{\nabla u}} \cdot \nabla h
+ \left( \lambda r + \alpha \nu'(u) \right) h \dx 
\\
&= \int_{\Gamma} h g \frac{\nabla u}{\abs{\nabla u}} \cdot \normal  \, d \Gamma - \int_{\Omega } \Div\left( g \frac{\nabla u}{\abs{\nabla u}} \right) - \lambda r - \alpha \nu'(u) \dx ,
\end{align*}
which gives us the descent equation 
\begin{align*}
u_t = \Div\left( g(x)  \frac{\nabla u}{\abs{\nabla u}} \right) - \lambda\big[ (c_1 - f)^2 - (c_2 - f)^2 \big]  - \alpha \nu'(u)
\end{align*}
and boundary condition $g\frac{\nabla u}{\abs{\nabla u}} \cdot \normal = 0$. 

Those familiar with exact penalty functions will note that there is a techical issue with the descent equation. As defined, $\nu(\xi)$ has two points of non-differentiability at $\xi = 0$ and $\xi = 1$. It will be necessary to substitute with a smooth penalty function, $\nu_\varepsilon(\xi)$. In our numerical examples we will be using a smoothed penalty function similar to one analyzed by Pinar and Zenios \cite{pinar1994smoothing}.



\subsection{The Split Bregman method}
In \cite{goldstein2010geometric}, Goldstein, Bresson and Osher applied the Split Bregman method \cite{goldstein2009split} to the GCS image segmentation model with good results. An advantage of using the Split Bregman method is that we are able to solve the minimization problem 
\begin{align}
\min_{0 \leq u \leq 1} \left\{ \egcs  
= \int_{\Omega } g\abs{\nabla u} \dx 
+ \lambda \int_{\Omega} \big[ (c_1 - f)^2 - (c_2 - f)^2\big] u \dx
\right\}
\label{eq:egcs1}
\end{align}
without the need of an exact penalty term or any added regularization of the TV-norm. We will compare and contrast this alternative to the earlier gradient descent approach. 

To derive the Split Bregman formulation, we'll first restate \eqref{eq:egcs1} in discrete form, define the auxiliary variable $d = (d^x, d^y) = (u_x, u_y) = \nabla u$ and introduce a quadratic penalty function to weakly enforce this equality. This gets us 
\begin{align*}
\min_{0\leq u \leq 1, d} 
 \norm{d }_g + \lambda  \langle r, u \rangle
+ \frac{\theta}{2} \norm{ d - \nabla u }_2^2,
\end{align*}
where $\norm{d}_g = \norm{g \nabla u}_1$. For problems such as ours \footnote{For more details, one can refer to the appendix, and \cite{goldstein2009split,goldstein2010geometric}}, the Bregman iteration simplifies to the 2-step procedure 
\begin{align}
(u^{k+1}, d^{k+1}) 
&= \argmin_{0\leq u \leq 1, d}  \norm{d}_g + \lambda \langle r, u \rangle + \frac{\theta}{2} \norm{d - \nabla u - b^k }^2_2 
\\
b^{k+1} &= b^k - (d^k - \nabla u^k) .
\label{eq:sbbij}
\end{align} 
The Split Bregman then breaks up the first step into 2 substeps that may be iterated until convergence, effectively decoupling $u$ and $d$: 
\begin{align}
u^{k+1} &= \argmin_{0\leq u \leq 1} \lambda \langle r, u \rangle
+ \frac{\theta}{2} \norm{d^k - \nabla u - b^k }^2_2,
\label{eq:splitu}
\\
d^{k+1} 
&= \argmin_d \norm{d}_g + \frac{\theta}{2} \norm{d - \nabla u^{k+1} - b^k}^2_2.
\label{eq:splitd}
\end{align}
Each of the substeps can be solved efficiently. We begin by tackling the solution to \eqref{eq:splitd}.

To begin, let's clarify precisely what the vector norm $\norm{\cdot }_g$ is as there is more than one choice. We will use the one referred to as isotropic TV in \cite{goldstein2009split},
\begin{align*}
\norm{ d }_g 
= \sum^N_{i,j = 1} g_{ij} \norm{d_{ij}}_2 
= \sum^{N}_{i,j = 1} g_{ij} \sqrt{ (d^x_{ij} )^2 + (d^y_{ij} )^2 }
\end{align*}
in which case \eqref{eq:splitd} is equivalently 
\begin{align*}
\sum^{N}_{i,j = 1} g_{ij} \sqrt{ (d^x_{ij} )^2 + (d^y_{ij} )^2 }
+ \frac{\theta}{2} \sum^N_{i,j=1} \big[ (d^x_{ij} - \nabla_x u^{k+1}_{ij} - (b^x_{ij})^k)^2 
+ (d^y_{ij} - \nabla_y u^{k+1}_{ij} - (b^y_{ij})^k)^2 
\big]
\eqqcolon \sum^N_{i,j=1} S_{ij}
\end{align*}
For each $(i,j)$ we can optimize by elementary calculus, 
\begin{align*}
0
=\pd{S_{i,j}}{d^x_{i,j}} 
= g_{ij}\frac{d^x_{ij}}{ \norm{d_{ij}}_2}
+ \theta (d^x_{ij} - \nabla_x u^{k+1}_{ij} - (b^x_{ij})^k) 
\iff
\nabla_x u^{k+1}_{ij} + (b^x_{ij})^k  
= d^x_{ij} \left( 1 + \frac{g_{ij}}{\theta \norm{d_{ij}}_2}
\right)
\\
0
=\pd{S_{ij}}{d^y_{i,j}} 
= g_{ij}\frac{d^y_{ij}}{ \norm{d_{ij}}_2}
+ \theta (d^y_{ij} - \nabla_y u^{k+1}_{ij} - (b^y_{ij})^k) 
\iff
\nabla_y u^{k+1}_{ij} + (b^y_{ij})^k 
= d^y_{ij} \left( 1 + \frac{g_{ij}}{\theta\norm{d_{ij}}_2}
\right)
\end{align*}
which combines as 
\begin{align*}
\norm{\nabla u^{k+1}_{ij} + b^k_{ij}}_2^2 = \left(\norm{ d_{ij}}_2 + \frac{g_{ij}}{\theta}
\right)^2
\implies 
\norm{d_{ij}}_2 = \norm{\nabla u^{k+1}_{ij} + b^k_{ij} }_2 - \frac{g_{ij}}{\theta},
\end{align*}
(if solution exists), so
\begin{align*}
d_{ij}^x 
= \frac{ \nabla_x u^{k+1}_{ij} + (b^x_{ij})^k }{ \norm{\nabla u^{k+1}_{ij} + b^k_{ij} }_2} \left( \norm{\nabla u^{k+1}_{ij} + b^k_{ij} }_2 - \frac{g_{ij}}{\theta} \right), 
\end{align*}
and similarly for $d^y_{ij}$. Otherwise, there is one other critical point giving the minimum at $d = 0$; the full solution is typically presented as 
\begin{align}
(d^x)^{k+1} 
= \max\left( \norm{ \nabla u^{k+1} + b^k}_2 - \frac{g}{\theta}, 0 \right) \frac{ \nabla_x u^{k+1} + (b^x)^k}{ \norm{ \nabla u^{k+1} + b^k}_2}
\label{eq:sbdij}
\end{align}
and likewise for $(d^y)^{k+1}$.

The solution to \eqref{eq:splitu} can be found by first differentiating to get $0 = \lambda  +\theta  \nabla \cdot ( d^k - \nabla u - b^k)$ which leads to $\Delta u = \frac{\lambda}{\theta} r + \nabla \cdot (d^k - b^k)$. As the energy is quadratic in $u_{ij}$, either the optimal value is given by this PDE, or if it were outside the interval $[0,1]$, then must take either $0$ or $1$, whichever is nearer the unconstrained minimizer. For example, with centred differencing of the Laplacian, we have 
\begin{align}
\widetilde u_{ij} = \frac{1}{4}\left(
u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} 
- \frac{\lambda}{\theta} r_{ij}
- \nabla_x ((d^x_{ij})^k - (b^x_{ij})^k)
- \nabla_y((d^y_{ij})^k - (b^y_{ij})^k)
\right) 
\label{eq:sbuij}
\end{align}
and 
\begin{align}
u_{ij}  = \max \{ \min\{ \widetilde u_{ij}, 1 \} , 0 \}.
\label{eq:sbuij2}
\end{align}
The full algorithm, to summarize, is
\begin{enumerate}
	\item Threshold $u^k$ to determine $\Sigma^k$, eg. $\Sigma^k = \{x\in\Omega \mid u(x) \geq 0.5 \}$.
	\item Update $c_1, c_2$ by \Cref{eq:c1,eq:c2} with $\Sigma^k$ in place of $\Sigma$.
	
	\item Evaluate $r = (c_1- f)^2 - (c_2 - f)^2$.

	\item Update $u^{k+1}$ by \Cref{eq:sbuij,eq:sbuij2} using one sweep of Gauss-Seidel.
	
	\item Update $d^{k+1} = ((d^x)^{k+1}, (d^y)^{k+1})$ by \Cref{eq:sbdij}.
	
	\item Update $b^{k+1} = ((b^x)^{k+1}, (b^y)^{k+1})$ by \Cref{eq:sbbij}
	
	\item Repeat from step 1 if $\norm{u^{k+1} - u^k} > \textrm{tol}$.
\end{enumerate}
With regards to step 4, the authors in \cite{goldstein2010geometric} recommend just one sweep of Gauss-Seidel rather than iterating to full convergence. The reason being that any advantage would be lost against the error in the updates to the Bregman parameter, $b^{k+1}$.

