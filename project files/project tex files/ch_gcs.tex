\chapter{Globally Convex Segmentation}
\label{ch:gcs}
Our numerical examples with the two models, GAC and ACWE, show still plenty of room for improvement from the modelling perspective. Of particular concern is this knack for finding and getting stuck at local minima (by design, in case of the GAC model), and therefore the solution we get is dependent on the initial contour we set. These issues are address by Chan, Esedo\={g}lu, and Nikolova \cite{chan2006algorithms}, and their ideas further refined by Bresson et al. \cite{bresson2007fast}, in what they referred to as the convexification of the ACWE model and the unification of two models we have considered.

In this chapter our viewpoint on the segmentation problem shifts somewhat. With the two previous models, the expression and evolution of the active contour, in the form of a level set function, was at the forefront. When we turn to the globally convex segmentation (GCS) model, the viewpoint shifts to subsets and indicator functions of these subsets. Keep in mind that for 2-phase segmentation any subset $\Sigma \subset \Omega$, and equivalently the corresponding indicator function $\ind_\Sigma$, defines a segmentation of the image domain. Thus the objective is to determine an indicator function $u(x) = \ind_\Sigma(x)$, and a piecewise constant solution $\widetilde u(x) = c_1 \ind_\Sigma(x) + c_2 (1 - \ind_\Sigma(x))$. 

First we address the non-convexity in the ACWE model. Restating the optimization problem in terms of indicator functions, we have 
\begin{align}
\min_{\substack{\Sigma\subset\Omega \\ 
		u(x) = \ind_\Sigma(x)}} 
\left\{\eacwe(u, c_1, c_2; \lambda)
= \int_{\Omega} \abs{\nabla u} \dx 
+ \int_\Omega u(c_1 - f)^2   + (1-u)(c_2 - f)^2 \dx 
\right\}.
\label{eq:eacwe2}
\end{align}
Observe that the function set we are optimizing over is not convex. For instance suppose $\Sigma_1, \Sigma_2 \subset \Omega$, $\Sigma_1 \cap \Sigma_2 = \emptyset$, $\Omega \setminus (\Sigma_1 \cup \Sigma_2) \neq \emptyset$ and set $u_1 = \ind_{\Sigma_1}$ and $u_2 = \ind_{\Sigma_2}$. Then any convex combination $w = ku_1 + (1-k)u_2$, $k \in (0,1)$, would be a function that takes on three values. Addressing the issue of convexity are two key theorems presented in the next section which relaxes the restriction on our function space, allows $u$ to take on the continuum of values between 0 and 1, and leads to an equivalent but convex minimization problem.

\section{A unified GAC \& ACWE convex segmentation model}
The key observation made by Chan et al. \cite{chan2006algorithms} is that in choosing a non-compactly support $H_\varepsilon$ for the gradient descent evolution \eqref{eq:acwe_el}, the following gradient flow will have the same steady state solutions: 
\begin{align*}
\phi_t = \Div\left(\frac{\nabla \phi}{\abs{\nabla \phi}} \right) 
- \lambda (c_1 - f)^2 + \lambda (c_2 - f)^2 .
\end{align*}
This in turn is the gradient descent equation the following energy:
\begin{align}
E(\phi, c_1, c_2) 
= \int_\Omega \abs{\nabla \phi} \dx 
+ \lambda \int_\Omega \big[ (c_1 - f)^2 - (c_2 - f)^2 \big] \phi \dx ,
\label{eq:mod_acwe}
\end{align}
which gets us to the first of two key theorems from \cite{chan2006algorithms}.

\begin{thm}
	For any given fixed $c_1, c_2 \in \mathbf{R}$, a global minimizer for $\eacwe(\cdot, c_1, c_2; \lambda)$ can be found by carrying out the following convex minimization: 
	\begin{align}
	\min_{0\leq u \leq 1}\left\{
		\widetilde E_{\textrm{ACWE}} (u, c_1, c_2; \lambda)
		=
		\int_{\Omega} \abs{\nabla u} \dx 
	+ \lambda\int_{\Omega} \big[ (c_1 - f(x) )^2 - (c_2 - f(x))^2 \big] u(x) \dx
	\right\}
	\label{eq:thm1}
	\end{align}
	and then setting $\Sigma = \{ x \mid u(x) \geq \mu \}$ for a.e. $\mu \in [0, 1]$.
	\label{thm:1}
\end{thm}
The theorem above links the two energies, $\eacwe$ and $\widetilde E_\textrm{ACWE}$,
and guarantees that any global minimum of \eqref{eq:thm1} is only a thresholding step away from a global minimum of the ACWE model. The next theorem is but one way to tackle the minimization problem \eqref{eq:thm1}.

\begin{thm}
	Let $r(x) \in L^\infty(\Omega)$. Then the convex, constrained minimization problem
	\begin{align*}
	\min_{0 \leq u \leq 1} \int_{\Omega} \abs{\nabla u} \dx + \lambda \int_{\Omega} r(x) u \dx 
	\end{align*}
	has the same set of minimizers as the following convex, unconstrained minimization problem:
	\begin{align*}
	\min_u \int_{\Omega} \abs{\nabla u} \dx + \lambda \int_{\Omega} r(x) u + \alpha \nu(u) \dx 
	\end{align*}
	where $\nu(z) = \max\{ 0 , 2\abs{z - \frac{1}{2} } - 1\}$, provided that $\alpha > \frac{\lambda}{2}\norm{r(x)}_{L^\infty(\Omega)}$.
	\label{thm:2}
\end{thm}
The addition of the exact penalty term \cite{hiriart1993convexI,hiriart1993convexII} grants us an unconstrained formulation. Deriving the Euler-Lagrange equation is straightforward in this form. But before doing so, we add one more modification to give a unified globally convex segmentation (GCS) model.

Per Bresson et al. \cite{bresson2007fast}, building on the work of \cite{chan2006algorithms}, they propose the minimization 
\begin{align}
\min_{0\leq u \leq 1} \left\{
\egcs(u, c_1, c_2; \lambda) 
= 
\int_{\Omega} g(x)  \abs{\nabla u} \dx 
+ \lambda\int_{\Omega} \big[ (c_1 - f(x) )^2 - (c_2 - f(x))^2 \big] u(x) \dx
\right\},
\end{align}
Note the similarity to \eqref{eq:thm1}. The only change is the added edge indicator function $g(x)$ in the first term. Note also the close relationship to $\egac$ from \Cref{ch:gac}. If $u$ is an indicator function, then 
\begin{align*}
\egcs(\ind_\Sigma(x), c_1, c_2; \lambda) 
&= \int_{\Omega} g(x)  \abs{\nabla \ind_\Sigma(x)} \dx 
+ \lambda\int_{\Omega} \big[ (c_1 - f(x) )^2 - (c_2 - f(x))^2 \big] \ind_\Sigma(x) \dx
\\
&= \egac(\partial \Sigma) +  \lambda\int_{\Omega} \big[ (c_1 - f(x) )^2 - (c_2 - f(x))^2 \big] \ind_\Sigma(x) \dx,
\end{align*}
reduces to the familiar GAC energy but now subject to an ACWE-type fitting energy constraint. Finally, we point out that $\egcs$ satisfies statements same as those in Theorem \ref{thm:1} and \ref{thm:2}, but with $\int g\abs{\nabla u}$ in place of $\int \abs{\nabla u}$.

The next section will not be just one, but two numerical algorithms for image segmentation under the GCS model. In addition to the usual Euler-Lagrange equation and gradient descent, we will also discuss application of the Split Bregman method \cite{goldstein2010geometric,goldstein2009split} to this segmentation model.

\section{Minimization of the GCS model: gradient descent and the Split Bregman}
For the following sections, let 
\begin{align*}
r(x, c_1, c_2) = (c_1 - f)^2 - (c_2 - f)^2.
\end{align*}

\subsection{Euler-Lagrange for the GCS model}
For this subsection, we are considering the unconstrained minimization problem 
\begin{align*}
\min_u \underbrace{\int_{\Omega } g\abs{\nabla u} \dx 
+ \int_{\Omega} \lambda r u + \alpha \nu(u) \dx }_{=\widetilde E_{\textrm{GCS}}(u)},
\end{align*}
($\egcs$ with an exact penalty term, see Theorem \ref{thm:2}).
Fixing $c_1, c_2$ and $\lambda$, we have
\begin{align*}
\dd{}{\gamma} \widetilde E_\textrm{GCS}(u + \gamma h)
\bigg\rvert_{\gamma = 0} 
&= \int_{\Omega} g \frac{\nabla u}{\abs{\nabla u}} \cdot \nabla h
+ \left( \lambda r + \alpha \nu'(u) \right) h \dx 
\\
&= \int_{\Gamma} h g \frac{\nabla u}{\abs{\nabla u}} \cdot \normal  \, d \Gamma - \int_{\Omega } \Div\left( g \frac{\nabla u}{\abs{\nabla u}} \right) - \lambda r - \alpha \nu'(u) \dx ,
\end{align*}
which gives us the descent equation 
\begin{align}
\label{eq:gcs_el}
u_t = \Div\left( g(x)  \frac{\nabla u}{\abs{\nabla u}} \right) - \lambda r(x,c_1,c_2)- \alpha \nu'(u)
\end{align}
and boundary condition $g\frac{\nabla u}{\abs{\nabla u}} \cdot \normal = 0$. 

Those familiar with exact penalty functions will note a small techical issue with the descent equation. As defined in Theorem \ref{thm:2}, $\nu(z)$ has two points of non-differentiability at $z = 0$ and $z = 1$. It will be necessary to substitute with a smooth penalty function, $\nu_\varepsilon(z)$. In our numerical examples, we use
\begin{align*}
\nu_\varepsilon(z) = \begin{cases}
-2z - \varepsilon/2, & z < -\frac{\varepsilon }{2}
\\
\frac{2}{\varepsilon} z^2, &-\frac{\varepsilon}{2} \leq z < 0
\\
0, & 0\leq z \leq 1
\\
\frac{2}{\varepsilon}(z-1)^2, & 1 < x \leq 1 + \frac{\varepsilon }{2}
\\
2(z - 1) - \frac{\varepsilon }{2}, & z > 1+\frac{\varepsilon }{2}.
\end{cases}
\end{align*}
This choice is similar to one analyzed by Pinar and Zenios \cite{pinar1994smoothing}.

The numerical discretization of \eqref{eq:gcs_el} is almost a carbon copy of our work in \Cref{sec:gac_num_disc,sec:3.2}. The Gauss-Seidel iterations are (if working through $(i,j)$ in the usual order)
\begin{align*}
u^{n+1}_{ij} 
= \frac{1}{a_0} 
\left( u^n_{ij} + \Delta t 
\left( a_1 u^n_{i+1,j} + a_2 u^{n+1}_{i-1,j} + a_3 u^{n}_{i,j+1} + a_4 u^{n+1}_{i,j-1}
- \lambda r_{ij}^n
- \alpha \nu'_\varepsilon( u^n_{ij} )
\right)
\right),
\end{align*}
with $a_1,a_2,a_3,a_4$ as given in \Cref{sec:gac_num_disc} and $a_0 = 1 + \Delta t(a_1 + a_2 + a_3 + a_4)$. Perhaps more important to discuss is the role of the thresholding step from Theorem \ref{thm:1}. Recall that we are using the alternating minimization scheme to first set $c_1$ and $c_2$, then minimizing $\egac(\cdot, c_1, c_2)$ to determine $u$. In practice, this means we set $\Sigma = \{x \mid u(x) \geq 0.5\}$ whenever it is necessary to update $c_1$ and $c_2$, and when presenting the steady state solution and the contour $\C$.


\subsection{The Split Bregman method}
In \cite{goldstein2010geometric}, Goldstein, Bresson and Osher applied the Split Bregman method \cite{goldstein2009split} to the GCS image segmentation model with good results. An advantage of using the Split Bregman method is that we are able to solve the minimization problem 
\begin{align}
\min_{0 \leq u \leq 1} \left\{ \egcs  
= \int_{\Omega } g\abs{\nabla u} \dx 
+ \lambda \int_{\Omega} \big[ (c_1 - f)^2 - (c_2 - f)^2\big] u \dx
\right\}
\label{eq:egcs1}
\end{align}
without the need of an exact penalty term or any added regularization of the TV-norm. We will compare and contrast this alternative to the earlier gradient descent approach. 

To derive the Split Bregman formulation, we'll first restate \eqref{eq:egcs1} in discrete form, define the auxiliary variable $d = (d^x, d^y) = (u_x, u_y) = \nabla u$ and introduce a quadratic penalty function to weakly enforce this equality. This gets us 
\begin{align*}
\min_{0\leq u \leq 1, d} 
 \norm{d }_g + \lambda  \langle r, u \rangle
+ \frac{\theta}{2} \norm{ d - \nabla u }_2^2,
\end{align*}
where $\norm{d}_g = \norm{g \nabla u}_1$. For problems such as ours \footnote{For more details, one can refer to the appendix, and \cite{goldstein2009split,goldstein2010geometric}}, the Bregman iteration simplifies to the 2-step procedure 
\begin{align}
(u^{k+1}, d^{k+1}) 
&= \argmin_{0\leq u \leq 1, d}  \norm{d}_g + \lambda \langle r, u \rangle + \frac{\theta}{2} \norm{d - \nabla u - b^k }^2_2 
\\
b^{k+1} &= b^k - (d^k - \nabla u^k) .
\label{eq:sbbij}
\end{align} 
The Split Bregman then breaks up the first step into 2 substeps that may be iterated until convergence, effectively decoupling $u$ and $d$: 
\begin{align}
u^{k+1} &= \argmin_{0\leq u \leq 1} \lambda \langle r, u \rangle
+ \frac{\theta}{2} \norm{d^k - \nabla u - b^k }^2_2,
\label{eq:splitu}
\\
d^{k+1} 
&= \argmin_d \norm{d}_g + \frac{\theta}{2} \norm{d - \nabla u^{k+1} - b^k}^2_2.
\label{eq:splitd}
\end{align}
Each of the substeps can be solved efficiently. We begin by tackling the solution to \eqref{eq:splitd}.

To begin, let's clarify precisely what the vector norm $\norm{\cdot }_g$ is as there is more than one choice. We will use the one referred to as isotropic TV in \cite{goldstein2009split},
\begin{align*}
\norm{ d }_g 
= \sum^N_{i,j = 1} g_{ij} \norm{d_{ij}}_2 
= \sum^{N}_{i,j = 1} g_{ij} \sqrt{ (d^x_{ij} )^2 + (d^y_{ij} )^2 }
\end{align*}
in which case \eqref{eq:splitd} is equivalently 
\begin{align*}
\sum^{N}_{i,j = 1} g_{ij} \sqrt{ (d^x_{ij} )^2 + (d^y_{ij} )^2 }
+ \frac{\theta}{2} \sum^N_{i,j=1} \big[ (d^x_{ij} - \nabla_x u^{k+1}_{ij} - (b^x_{ij})^k)^2 
+ (d^y_{ij} - \nabla_y u^{k+1}_{ij} - (b^y_{ij})^k)^2 
\big]
\eqqcolon \sum^N_{i,j=1} S_{ij}
\end{align*}
For each $(i,j)$ we can optimize by elementary calculus, 
\begin{align*}
0
=\pd{S_{i,j}}{d^x_{i,j}} 
= g_{ij}\frac{d^x_{ij}}{ \norm{d_{ij}}_2}
+ \theta (d^x_{ij} - \nabla_x u^{k+1}_{ij} - (b^x_{ij})^k) 
\iff
\nabla_x u^{k+1}_{ij} + (b^x_{ij})^k  
= d^x_{ij} \left( 1 + \frac{g_{ij}}{\theta \norm{d_{ij}}_2}
\right)
\\
0
=\pd{S_{ij}}{d^y_{i,j}} 
= g_{ij}\frac{d^y_{ij}}{ \norm{d_{ij}}_2}
+ \theta (d^y_{ij} - \nabla_y u^{k+1}_{ij} - (b^y_{ij})^k) 
\iff
\nabla_y u^{k+1}_{ij} + (b^y_{ij})^k 
= d^y_{ij} \left( 1 + \frac{g_{ij}}{\theta\norm{d_{ij}}_2}
\right)
\end{align*}
which combines as 
\begin{align*}
\norm{\nabla u^{k+1}_{ij} + b^k_{ij}}_2^2 = \left(\norm{ d_{ij}}_2 + \frac{g_{ij}}{\theta}
\right)^2
\implies 
\norm{d_{ij}}_2 = \norm{\nabla u^{k+1}_{ij} + b^k_{ij} }_2 - \frac{g_{ij}}{\theta},
\end{align*}
(if solution exists), so
\begin{align*}
d_{ij}^x 
= \frac{ \nabla_x u^{k+1}_{ij} + (b^x_{ij})^k }{ \norm{\nabla u^{k+1}_{ij} + b^k_{ij} }_2} \left( \norm{\nabla u^{k+1}_{ij} + b^k_{ij} }_2 - \frac{g_{ij}}{\theta} \right), 
\end{align*}
and similarly for $d^y_{ij}$. Otherwise, there is one other critical point giving the minimum at $d = 0$; the full solution is typically presented as 
\begin{align}
(d^x)^{k+1} 
= \max\left( \norm{ \nabla u^{k+1} + b^k}_2 - \frac{g}{\theta}, 0 \right) \frac{ \nabla_x u^{k+1} + (b^x)^k}{ \norm{ \nabla u^{k+1} + b^k}_2}
\label{eq:sbdij}
\end{align}
and likewise for $(d^y)^{k+1}$.

The solution to \eqref{eq:splitu} can be found by first differentiating to get $0 = \lambda  +\theta  \nabla \cdot ( d^k - \nabla u - b^k)$ which leads to $\Delta u = \frac{\lambda}{\theta} r + \nabla \cdot (d^k - b^k)$. As the energy is quadratic in $u_{ij}$, either the optimal value is given by this PDE, or if it were outside the interval $[0,1]$, then must take either $0$ or $1$, whichever is nearer the unconstrained minimizer. For example, with centred differencing of the Laplacian, we have 
\begin{align}
\widetilde u_{ij} = \frac{1}{4}\left(
u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} 
- \frac{\lambda}{\theta} r_{ij}
- \nabla_x ((d^x_{ij})^k - (b^x_{ij})^k)
- \nabla_y((d^y_{ij})^k - (b^y_{ij})^k)
\right) 
\label{eq:sbuij}
\end{align}
and 
\begin{align}
u_{ij}  = \max \{ \min\{ \widetilde u_{ij}, 1 \} , 0 \}.
\label{eq:sbuij2}
\end{align}
The full algorithm, to summarize, is
\begin{enumerate}
	\item Threshold $u^k$ to determine $\Sigma^k$, eg. $\Sigma^k = \{x\in\Omega \mid u(x) \geq 0.5 \}$.
	\item Update $c_1, c_2$ by \Cref{eq:c1,eq:c2} with $\Sigma^k$ in place of $\Sigma$.
	
	\item Evaluate $r = (c_1- f)^2 - (c_2 - f)^2$.

	\item Update $u^{k+1}$ by \Cref{eq:sbuij,eq:sbuij2} using one sweep of Gauss-Seidel.
	
	\item Update $d^{k+1} = ((d^x)^{k+1}, (d^y)^{k+1})$ by \Cref{eq:sbdij}.
	
	\item Update $b^{k+1} = ((b^x)^{k+1}, (b^y)^{k+1})$ by \Cref{eq:sbbij}
	
	\item Repeat from step 1 if $\norm{u^{k+1} - u^k} > \textrm{tol}$.
\end{enumerate}
With regards to step 4, the authors in \cite{goldstein2010geometric} recommend just one sweep of Gauss-Seidel rather than iterating to full convergence. The reason being that any advantage would be lost against the error in the updates to the Bregman parameter, $b^{k+1}$.

