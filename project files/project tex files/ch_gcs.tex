\chapter{Globally Convex Segmentation}
Numerical experiments in the two models, GAC and ACWE, show that there is certainly room for improvement from the modelling perspective. Of particular concern are the models knack for finding and getting stuck at local minima (by design, in case of the GAC model), and therefore the solution attained is dependent on the initial contour. These issues are address by Chan, Esedo\={g}lu, and Nikolova \cite{chan2006algorithms}, and their ideas further refined by Bresson et al. \cite{bresson2007fast}, in what they referred to as the convexification of the ACWE model and the unification of two models we have considered.

Before going in depth, we should explain that in this chapter our viewpoint on the segmentation problem shifts somewhat. In the previous models, the viewpoint was mainly on an active contours and its evolution. However in the GCS model, the viewpoint is centred about indicator functions (of sets). Keep in mind that for 2-phase segmentation, any subset $\Sigma \subset \Omega$, and equivalently the corresponding indicator function $\ind_\Sigma$, defines a segmentation of the image domain. In what follows, the goal will arriving at an indicator function $u(x) = \ind_\Sigma(x)$, and the piecewise constant solution $\widetilde u(x) = c_1 \ind_\Sigma(x) + c_2 (1 - \ind_\Sigma(x))$. In other words, the problem may instead be framed as optimizating over functions that take only two values to find the best approximation to a given image $f$.

But first it is important to understand the nature of the non-convexity in the ACWE model. Restating the optimization problem in terms of indicator functions, we have 
\begin{align}
\min_{\substack{\Sigma\subset\Omega \\ 
		u(x) = \ind_\Sigma(x)}} 
\left\{\eacwe(u, c_1, c_2; \lambda)
= \int_{\Omega} \abs{\nabla u} \dx 
+ \int_\Omega u(c_1 - f)^2   + (1-u)(c_2 - f)^2 \dx 
\right\}.
\label{eq:eacwe2}
\end{align}
Observe that the function set we are optimizing over is not convex. For instance suppose $\Sigma_1, \Sigma_2 \subset \Omega$, $\Sigma_1 \cap \Sigma_2 = \emptyset$, $\Omega \setminus (\Sigma_1 \cup \Sigma_2) \neq \emptyset$ and set $u_1 = \ind_{\Sigma_1}$ and $u_2 = \ind_{\Sigma_2}$. Then any convex combination $w = ku_1 + (1-k)u_2$, $k \in (0,1)$, would be a function that takes on three values. 

In the next section are two key theorems which will allow that constraint to be relaxed, i.e. allow $u$ to take on the continuum of values between 0 and 1, and lead to an equivalent but convex minimization problem.

\section{A unified GAC \& ACWE convex segmentation model}
We first start the key observation that in choosing a non-compactly support $\delta_\epsilon$ in the gradient descent evolution \eqref{eq:acwe_el}, the following will have the same steady state solutions: 
\begin{align*}
\phi_t = \Div\left(\frac{\nabla \phi}{\abs{\nabla \phi}} \right) 
- \lambda (c_1 - f)^2 + \lambda (c_2 - f)^2 .
\end{align*}
This in turn is the gradient descent equation the following energy:
\begin{align}
E(\phi, c_1, c_2) 
= \int_\Omega \abs{\nabla \phi} \dx 
+ \lambda \int_\Omega \big[ (c_1 - f)^2 - (c_2 - f)^2 \big] \phi \dx ,
\label{eq:mod_acwe}
\end{align}
which gets us to the first of two key theorems from \cite{chan2006algorithms}.

\begin{thm}
	For any given fixed $c_1, c_2 \in \mathbf{R}$, a global minimizer for $\eacwe(\cdot, c_1, c_2; \lambda)$ can be found by carrying out the following convex minimization: 
	\begin{align}
	\min_{0\leq u \leq 1}\left\{
		\widetilde E_{\textrm{ACWE}} (u, c_1, c_2; \lambda)
		=
		\int_{\Omega} \abs{\nabla u} \dx 
	+ \lambda\int_{\Omega} \big[ (c_1 - f(x) )^2 - (c_2 - f(x))^2 \big] u(x) \dx
	\right\}
	\label{eq:thm1}
	\end{align}
	and then setting $\Sigma = \{ x \mid u(x) \geq \mu \}$ for a.e. $\mu \in [0, 1]$.
	\label{thm:1}
\end{thm}
The theorem above links the two energies, $\eacwe$ and $\widetilde E_\textrm{ACWE}$,
and guarantees that any global minimum of \eqref{eq:mod_acwe} is only a thresholding step away from a global minimum of the ACWE model. The next theorem is but one way to tackle \eqref{eq:thm1}.

\begin{thm}
	Let $r(x) \in L^\infty(\Omega)$. Then the convex, constrained minimization problem
	\begin{align*}
	\min_{0 \leq u \leq 1} \int_{\Omega} \abs{\nabla u} \dx + \lambda \int_{\Omega} r(x) u \dx 
	\end{align*}
	has the same set of minimizers as the following convex, unconstrained minimization problem:
	\begin{align*}
	\min_u \int_{\Omega} \abs{\nabla u} \dx + \lambda \int_{\Omega} r(x) u + \alpha \nu(u) \dx 
	\end{align*}
	where $\nu(\xi) = \max\{ 0 , 2\abs{\xi - \frac{1}{2} } - 1\}$, provided that $\alpha > \frac{\lambda}{2}\norm{r(x)}_{L^\infty(\Omega)}$.
	\label{thm:2}
\end{thm}
The term $\alpha\nu(u)$ is an exact penalty term \cite{hiriart1993convexI,hiriart1993convexII}. The advantage of this new unconstrained formulation is that it is quite straightforward to derive the Euler-Lagrange equation and solve by gradient descent. But before doing so, we will add one more modification to give a unified globally convex segmentation (GCS) model.

Per Bresson et al. \cite{bresson2007fast}, building on the work of \cite{chan2006algorithms}, they propose minimization of the energy 
\begin{align}
\egcs(u, c_1, c_2; \lambda) = 
\int_{\Omega} g(x)  \abs{\nabla u} \dx 
+ \lambda\int_{\Omega} \big[ (c_1 - f(x) )^2 - (c_2 - f(x))^2 \big] u(x) \dx,
\end{align}
where $g$ is an edge indicator function as in \Cref{ch:gac}. Furthermore, if $u$ is an indicator function, then
\begin{align*}
\egcs(\ind_\Sigma(x), c_1, c_2; \lambda) 
&= \int_{\Omega} g(x)  \abs{\nabla \ind_\Sigma(x)} \dx 
+ \lambda\int_{\Omega} \big[ (c_1 - f(x) )^2 - (c_2 - f(x))^2 \big] \ind_\Sigma(x) \dx
\\
&= \egac(\partial \Sigma) +  \lambda\int_{\Omega} \big[ (c_1 - f(x) )^2 - (c_2 - f(x))^2 \big] \ind_\Sigma(x) \dx
\end{align*}
reduces to the familiar GAC energy but now subject to an ACWE-type fitting energy constraint. Finally, also note that $\egcs$ satisfies statements same as those in Theorem \ref{thm:1} and \ref{thm:2}, but with $\int g\abs{\nabla u}$ in place of $\int \abs{\nabla u}$.

The next section will not be just one, but two numerical algorithms for image segmentation under the GCS model. In addition to the usual Euler-Lagrange equation and gradient descent, we will also discuss application of the Split Bregman method \cite{goldstein2010geometric,goldstein2009split} to this segmentation model.

\section{Minimization of the GCS model: gradient descent and the Split Bregman}
\subsection{Euler-Lagrange for the GCS model}
For this subsection, we are considering the unconstrained minimization problem 
\begin{align*}
\min_u \underbrace{\int_{\Omega } g\abs{\nabla u} \dx 
+ \int_{\Omega} \lambda \overbrace{\big[ (c_1 - f)^2 - (c_2 - f)^2 \big]}^{=r(x)} u + \alpha \nu(u) \dx }_{=\widetilde E_{\textrm{GCS}}(u)}
\end{align*}
($\egcs$ with an exact penalty term, see Theorem \ref{thm:2}).
Fixing $c_1, c_2$ and $\lambda$, we have
\begin{align*}
\dd{}{\gamma} \widetilde E_\textrm{GCS}(u + \gamma h)
\bigg\rvert_{\gamma = 0} 
&= \int_{\Omega} g \frac{\nabla u}{\abs{\nabla u}} \cdot \nabla h
+ \left( \lambda r + \alpha \nu'(u) \right) h \dx 
\\
&= \int_{\Gamma} h g \frac{\nabla u}{\abs{\nabla u}} \cdot \normal  \, d \Gamma - \int_{\Omega } \Div\left( g \frac{\nabla u}{\abs{\nabla u}} \right) - \lambda r - \alpha \nu'(u) \dx ,
\end{align*}
which gives us the descent equation 
\begin{align*}
u_t = \Div\left( g(x)  \frac{\nabla u}{\abs{\nabla u}} \right) - \lambda\big[ (c_1 - f)^2 - (c_2 - f)^2 \big]  - \alpha \nu_\varepsilon'(u)
\end{align*}
and boundary condition $g\frac{\nabla u}{\abs{\nabla u}} \cdot \normal = 0$. (We've also snuck in a regularized version of $\nu'$ as the $\nu(\xi)$ has two points of non-differentiability at $\xi = 0$ and $\xi = 1$.)

\subsection{The Split Bregman method}
