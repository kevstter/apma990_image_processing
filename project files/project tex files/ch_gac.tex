\chapter{Geodesic Active Contours}
\label{ch:gac}
The first segmentation model we review is the geodesic active contours (GAC) model from Caselles, Kimmel, and Sapiro \cite{caselles1997geodesic}. Their model of segmentation is the (near) automaticatic selection of an object or objects from an image once an initial contour $\C_0$ is prescribed. To get an active contour $\C = \C(t)$ with $\C(0) = \C_0$, they argued for the minimization of the energy 
\begin{align}
E_\mathrm{GAC}(\C) = \int^{L(\C)}_0 g\left( \abs{\nabla f } \right) \, ds,
\label{eq:egac}
\end{align}
where 
\begin{align*}
f&: \textrm{initial image},
\\
L(\C) &: \textrm{the length of the contour $\C$,} 
\\ 
g(\xi)&: \textrm{an edge indicator function}.
\end{align*}
The energy $E_\textrm{GAC}$ may be interpreted as a weighted arc length. If $g(\xi) = 1$, we would recognize $\int_\C  ds$ as standard Euclidean arc length. Hence by designing $g$ is be small near edges and large over homogeneous regions, one would expect the contour $\C$ to be drawn to and remain at object edges when minimizing $E_\textrm{GAC}$. As an example, one may set $g$ as 
\begin{align*}
g_1(\xi) 
= \frac{1}{1 + \alpha\xi^2},
\quad\text{or}\quad 
g_2(\xi) 
= \exp(-\beta\xi^2).
\end{align*}
Notice that as $\abs{\xi} \rightarrow \infty$, $g_1,g_2 \rightarrow 0$.  

In the next section, we will derive the Euler-Lagrange equation and minimize $\egac$ by gradient descent. However, before we move on, let us note that we are seeking local minima when minimizing $\egac$ as this energy does have the unfortunate feature that it attains a global minimum of zero when the contour contracts to a point and vanishes. We will revisit this issue when reviewing the results of the numerical examples.


\section{Euler-Lagrange equation to the GAC model}
The derivation will be helped tremendously with some setup and a quick review of vector calculus. Let $\C: [0,1]\rightarrow \mathbf{R}^2$ be a parametrized curve, $\C = \C(p)$. We then can relate the arc length element to the parameter $p$ as $ds = \abs{C'(p)} dp$. We also want to recognize the unit tangent vector, $\tang$, unit (inward) normal, $\normal$, and curvature, $\kappa$, as 
\begin{align*}
\tang 
= \frac{\C'}{\abs{\C'}},\quad 
\normal 
= \frac{ \tang '}{\abs{\tang'}},
\quad\text{and}\quad 
\kappa = \frac{ \abs{\tang'} }{ \abs{ \C' } },
\end{align*}
with the prime notation denoting differentiation w.r.t. $p$. We will also assume $\C(0) = \C(1)$. 

Next, rewriting $\egac$ as 
\begin{align*}
\egac = \int^{L(\C)}_0 g \, ds 
= \int^1_0 g(\C) \abs{\C'(p)} \, dp ,
\end{align*} 
we can compute its first variation: 
\begin{align*}
\dd{}{\gamma} \int^1_0 g(\C + \gamma h) \abs{\C' + \gamma h'} \, dp \bigg\rvert_{\gamma = 0}
&=  \int^1_0 \nabla g(\C) \cdot h \abs{\C'} + g(\C) \frac{\C'}{\abs{\C'}} \cdot h' \,dp 
\\
&=\int^1_0 \nabla g(\C) \cdot h \abs{\C'} + g(\C) \tang \cdot h' \,dp 
\\
&=\int^1_0 \nabla g \cdot h \abs{\C'} - ( g \tang)' \cdot h \, dp
\\
&=\int^1_0 \nabla g \cdot h \abs{\C'} - (\nabla g \cdot \C')(\tang \cdot h) - g\tang ' \cdot h \, dp
\\
&=\int^1_0 \nabla g \cdot h \abs{\C'} - (\nabla g \cdot \tang \abs{\C'} )(\tang \cdot h) - g\abs{\tang'} \normal \cdot h \, dp
\\
&=\int^1_0 \big[ \nabla g  - (\nabla g \cdot \tang )\tang \big] \cdot h \abs{\C'} - g\kappa \abs{\C'} \normal \cdot h \, dp
\\
&=\int^1_0 \big[\nabla g \cdot \normal \normal \big] \cdot h \abs{\C'} - g\kappa  \normal \cdot h \abs{\C'} \, dp
\\
&=\int^1_0 \big[ \nabla g \cdot \normal - g\kappa \big] \normal  \cdot h \abs{\C'} \, dp.
\\ 
\end{align*}
This gives a gradient descent evolution equation $\C_t = \left( g(\C) \kappa - \nabla g(\C) \cdot \normal \right) \normal $.
For the level set formulation, with $\phi = \phi(x, t)$ as the level set function, we have 
\begin{align}
\begin{split} 
\phi_t 
&= \left(
g( \abs{\nabla I } ) \Div\left( \frac{\nabla \phi}{\abs{\nabla \phi}} \right)
	+  \nabla g(  \abs{\nabla I } )\cdot  \frac{\nabla \phi}{\abs{\nabla \phi}}
\right) \abs{ \nabla \phi }
\\
&= \abs{\nabla \phi} \Div\left( 
g(  \abs{\nabla I } ) \frac{\nabla \phi}{\abs{\nabla \phi}}
\right) .
\end{split}
\label{eq:gac_ls}
\end{align}

\section{Numerical discretization}
\label{sec:gac_num_disc}
To discretize \eqref{eq:gac_ls}, we will use the semi-implicit Gauss-Seidel  numerical scheme proposed by Aubert and Vese \cite{aubert1997variational} and used to good effect by Chan and Vese with their ACWE model \cite{chan2001active} rather than using explicit forward Euler with second order centred differences as originally suggested\footnote{Although to be fair, it was suggested for ease of implementation as a proof of concept. For us, the same semi-implicit numerical scheme can and will be used again in \Cref{ch:acwe,ch:gcs} so it makes sense to give a complete presentation once and be able to reuse essentially the same code three times. } in \cite{caselles1997geodesic}.

Define the discrete differential operators: $D_x^+ u_{ij} = (u_{i+1,j} - u_{ij})/h$, $D^0_x = (u_{i+1,j} - u_{i-1,j})/(2h)$,
$D^-_x u_{ij} = (u_{ij} - u_{i-1,j})/h$, and similarly for $D^+_y, D^0_y, D^-_y$.  We will also let $\abs{D^0 \phi^n} = \sqrt{(D^0_x \phi^n_{ij})^2 + (D^0_y \phi^n_{ij})^2 } $ and $\varepsilon > 0$ be a small regularization parameter. 
Then discretize \eqref{eq:gac_ls} as 
\begin{align*}
\frac{\phi^{n+1}_{ij} - \phi^n_{ij}}{\Delta t} 
&= \abs{D^0 \phi^n}
\left(
D^-_x \left( \frac{g_{ij}D^+_x \phi_{ij}^{n+1}}{\sqrt{ (D^+_x \phi^n_{ij})^2 + (D^0_y \phi^n_{ij})^2 + \varepsilon^2}}
\right) 
+ D^-_y \left(  \frac{g_{ij}D^+_y \phi_{ij}^{n+1}}{\sqrt{ (D^0_x \phi^n_{ij})^2 + (D^+_y \phi^n_{ij})^2  + \varepsilon^2}}
\right)
\right)
\\
&= 
\abs{D^0 \phi^n}\frac{ g_{ij}/h^2 }{\sqrt{ \frac{1}{h^2}(\phi^n_{i+1j} -\phi^n_{ij} )^2 + \frac{1}{4h^2} (\phi^n_{i,j+1} - \phi^n_{i,j-1})^2 + \varepsilon^2}}
(\phi^{n+1}_{i+1,j} - \phi^{n+1}_{ij})
\\
&\quad-\abs{D^0 \phi^n}\frac{ g_{i-1,j}/h^2 }{\sqrt{ \frac{1}{h^2}(\phi^n_{ij} -\phi^n_{i-1,j} )^2 + \frac{1}{4h^2} (\phi^n_{i-1,j+1} - \phi^n_{i-1,j-1})^2 + \varepsilon^2}}
(\phi^{n+1}_{ij} - \phi^{n+1}_{i-1,j}) 
\\
&\quad+\abs{D^0 \phi^n}\frac{ g_{ij}/h^2 }{\sqrt{ \frac{1}{4h^2}(\phi^n_{i+1,j} -\phi^n_{i-1,j} )^2 + \frac{1}{h^2} (\phi^n_{i,j+1} - \phi^n_{ij})^2 + \varepsilon^2}}
(\phi^{n+1}_{i,j+1} - \phi^{n+1}_{ij}) 
\\
&\quad-\abs{D^0 \phi^n}\frac{ g_{i,j-1}/h^2 }{\sqrt{ \frac{1}{4h^2}(\phi^n_{i+1,j-1} -\phi^n_{i-1,j-1} )^2 + \frac{1}{h^2} (\phi^n_{ij} - \phi^n_{i,j-1})^2 + \varepsilon^2}}
(\phi^{n+1}_{ij} - \phi^{n+1}_{i,j-1}) 
\\
&\eqqcolon \abs{D^0 \phi^n} \big[ a_1(\phi^{n+1}_{i+1,j} - \phi^{n+1}_{ij})
- a_2(\phi^{n+1}_{ij} - \phi^{n+1}_{i-1,j}) 
+ a_3(\phi^{n+1}_{i,j+1} - \phi^{n+1}_{ij}) 
- a_4(\phi^{n+1}_{ij} - \phi^{n+1}_{i,j-1})
\big]
\\
&= \abs{D^0 \phi^n} 
\big[ a_1\phi^{n+1}_{i+1,j} 
+ a_2\phi^{n+1}_{i-1,j} 
+ a_3\phi^{n+1}_{i,j+1} 
+ a_4\phi^{n+1}_{i,j-1}
- (a_1 + a_2 + a_3 + a_4) \phi^{n+1}_{ij}
\big].
\end{align*}
Hence 
\begin{align*}
\big[ 
\underbrace{1 + \Delta t\abs{D^0 \phi^n} (a_1 + a_2 + a_3 + a_4) \big]
}_{\eqqcolon a_0} \phi^{n+1}_{ij} 
= \phi^n_{ij} + \Delta t\abs{D^0 \phi^n} (a_1\phi^{n+1}_{i+1,j} 
+ a_2\phi^{n+1}_{i-1,j} 
+ a_3\phi^{n+1}_{i,j+1} 
+ a_4\phi^{n+1}_{i,j-1}).
\end{align*}
The Gauss-Seidel iterations would be (if working through $(i,j)$ in the usual order)
\begin{align*}
\phi^{n+1}_{ij} 
= \frac{1}{a_0} 
\left( \phi^n_{ij} + \Delta t \abs{D^0 \phi^n}
\left( a_1 \phi^n_{i+1,j} + a_2 \phi^{n+1}_{i-1,j} + a_3 \phi^{n}_{i,j+1} + a_4 \phi^{n+1}_{i,j-1}
\right)
\right).
\end{align*}
One final note with regards to this numerical discretization. It is suggested to alternate the discretization of the divergence term, eg. applying $D^+_x$ on the outside and $D^-_x$ inside, and with various combinations with $D^+_y$, $D^-_y$ as well. Our testing shows this alternating reduces asymmetries in our solutions to the GAC model but produces no observable differences with our later segmentation models.


\section{Examples and discussion} 
In this section, parameters are $\Delta t = 0.05$, $h = 1$, and $\varepsilon = 10^{-6}$, unless otherwise stated.
The edge indicator function used is 
\begin{align*}
g = \frac{1}{1 + \abs{\nabla \widetilde f}^2 },
\end{align*}
where $\widetilde f$ is a Gaussian-smoothed version of the image $f$. Modification of the GAC model to include a constant velocity, $c$, in the normal direction, 
\begin{align}
\phi_t 
= \left( g ( c + \kappa) + \nabla g \cdot \frac{\nabla \phi}{\abs{\nabla \phi}} \right) \abs{\nabla \phi}
= 
\abs{\nabla \phi} \Div\left( g \frac{\nabla \phi}{\abs{\nabla \phi}} \right) 
+ cg \abs{\nabla \phi},
\label{eq:gac+c}
\end{align}
and is explored as well. See the examples for more detail.


In our first series of tests is a synthetic image with 8 objects (squares). We run four scenarios with this image. The two rows of \Cref{fig:grid} represent two different initial contours. For the top row, the initial contour is a single, giant circle, and from this initial state we run two cases.  Not adding a constant normal velocity, i.e. $c = 0$, we get a bounding box on our eight squares. The contour does not drive into the gaps. Supplied with an extra push, i.e. setting $c = -1$, the segmentation algorithm picks out all eight squares.
	
In the second row, we have multiple circles forming the initial contour, with two of the circles enclosing one square each and the centre circle feeling a little empty. Setting $c = -1$, the contours contract, causing the centre circle to vanish and the other two to wrap tightly around their square. Setting $c = 1$, the contours expand, missing two squares.
	
We find the model adheres to edges nicely. Whenever the contour locates an edge, it will stay there. However, even with limited testing it is clear that this method requires significant high level input in at least two ways. Firstly, the ``right'' initial contour is needed to generate the desired result. That may mean, for every image, a user having to drawing a rough contour around all objects before letting the algorithm take over. Second would be to specify $c$. Not only can nonzero values of $c$ produce drastically different results from $c = 0$, but it also drove the solution to steady state much quicker. The total number of iterations was reduced by a factor of over $100$ in some cases.
	
	\begin{figure}[htb!]
		\centering
		\begin{minipage}{0.31\textwidth}
					\includegraphics[width=\textwidth]{grid1}
		\end{minipage}%
		\begin{minipage}{0.31\textwidth}
			\includegraphics[width=\textwidth]{grid1ssc0}
		\end{minipage}%
		\begin{minipage}{0.31\textwidth}
			\includegraphics[width=\textwidth]{grid1sscm1}
		\end{minipage}
		\begin{minipage}{0.31\textwidth}
			\includegraphics[width=\textwidth]{grid2}
		\end{minipage}%
		\begin{minipage}{0.31\textwidth}
			\includegraphics[width=\textwidth]{grid2ssc0}
		\end{minipage}%
		\begin{minipage}{0.31\textwidth}
			\includegraphics[width=\textwidth]{grid2sscp1}
		\end{minipage}
		
		\caption{(Left) Initial setup. (Middle) Steady state solution, $c = 0$. (Top right) Steady state solution, $c = -1$. (Bottom right) Steady state solution, $c = 1$.}\label{fig:grid}
	\end{figure}



In closing this chapter, we recognize that while our one example is by no means a comprehensive study on this model, we have identified a key property we fully expect the next segmentation models to improve upon, namely sensitivity to the initial condition and reliance on high level input.
	

